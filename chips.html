<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AI Chip & Foundry Map — Edge · Datacenter · Outliers</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    :root {
      --bg: #050510;
      --fg: #f3f3ff;
      --fg-dim: #a0a0c5;
      --fg-muted: #7676a2;
      --border: #23233b;
      --border-subtle: #1b1b2d;
      --accent1: #7c5cff;
      --accent2: #2ac1ff;
      --radius: 10px;
      --radius-lg: 14px;
      --font: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      background: radial-gradient(circle at top, #101024 0%, #050510 40%, #02020a 100%);
      color: var(--fg);
      font-family: var(--font);
      -webkit-font-smoothing: antialiased;
    }
    header {
      padding: 26px 34px 20px;
      background: #111122;
      border-bottom: 1px solid #1f1f35;
    }
    h1 { margin: 0 0 8px; font-size: 26px; }
    .subtitle {
      margin: 0;
      color: var(--fg-dim);
      max-width: 880px;
      font-size: 14px;
      line-height: 1.6;
    }
    .badge {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 5px 12px;
      border-radius: 999px;
      border: 1px solid #353557;
      background: #0b0b18;
      font-size: 11px;
      color: #c5c5e5;
      text-transform: uppercase;
      letter-spacing: .09em;
      margin-bottom: 10px;
    }
    .badge-dot {
      width: 9px; height: 9px;
      border-radius: 50%;
      background: radial-gradient(circle, #a685ff, #6230ff);
      box-shadow: 0 0 9px rgba(160,120,255,0.9);
    }
    .container { padding: 22px 34px 36px; }
    .tabs {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-bottom: 16px;
    }
    .tab-btn {
      padding: 8px 14px;
      border-radius: 20px;
      border: 1px solid #2b2b48;
      background: #101020;
      color: #c5c5e5;
      font-size: 11px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      cursor: pointer;
    }
    .tab-btn.active {
      background: linear-gradient(135deg, var(--accent1), var(--accent2));
      color: #050510;
      border-color: transparent;
    }
    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-bottom: 10px;
    }
    .search-input {
      flex: 1;
      min-width: 220px;
      padding: 9px 12px;
      background: #0b0b19;
      border-radius: 6px;
      border: 1px solid #262644;
      color: var(--fg);
      font-size: 13px;
    }
    .search-input::placeholder { color: var(--fg-muted); }
    .export-btn {
      padding: 8px 14px;
      background: #7c5cff22;
      border: 1px solid #7c5cff;
      border-radius: 6px;
      color: #e4dbff;
      cursor: pointer;
      white-space: nowrap;
      font-size: 12px;
    }
    .hint { font-size: 11px; color: var(--fg-muted); margin-bottom: 8px; }
    .table-wrapper {
      overflow: auto;
      border-radius: var(--radius-lg);
      border: 1px solid var(--border-subtle);
      background: radial-gradient(circle at top, #06061a 0%, #040416 50%, #040416 100%);
      box-shadow: inset 0 0 0 1px rgba(20,20,40,0.5);
      max-height: 540px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      min-width: 1500px;
      table-layout: fixed;
      font-size: 0.76rem;
      color: var(--fg);
    }
    thead {
      background: #181831;
      position: sticky;
      top: 0;
      z-index: 5;
    }
    th, td {
      padding: 7px 8px;
      border-bottom: 1px solid rgba(30,40,60,0.9);
      white-space: normal;
      overflow-wrap: break-word;
    }
    th { font-size: 11px; color: #a0a0c8; text-transform: uppercase; }
    th span.small { font-size: 10px; color: #7b7ba5; font-weight: 400; }
    th:not(.notes), td:not(.notes) { width: 90px; }
    .notes {
      width: 360px;
      max-width: 400px;
      white-space: normal;
    }
    tbody tr:nth-child(even) td { background: #0c0c18; }
    tbody tr:nth-child(odd) td { background: #070714; }
    tbody tr:hover td { background: rgba(124,92,255,0.16); }
    .section { display: none; margin-top: 4px; }
    .section.active { display: block; }
    .section-title { font-size: 16px; margin: 0 0 4px; }
    .section-subtitle { margin: 0 0 10px; font-size: 13px; color: var(--fg-dim); }
    .chip-pill {
      display: inline-block;
      padding: 2px 7px;
      border-radius: 999px;
      border: 1px solid #333355;
      background: #151530;
      font-size: 11px;
    }
    .tag-foundry {
      display: inline-block;
      padding: 2px 6px;
      border-radius: 999px;
      border: 1px solid #3a3a60;
      background: #151528;
      font-size: 10px;
      color: #d8d8ff;
    }
    .tag-foundry.tsmc { border-color: #f97316; color: #fed7aa; }
    .tag-foundry.samsung { border-color: #f472b6; color: #fecdd3; }
    .tag-foundry.intel { border-color: #60a5fa; color: #bfdbfe; }
    #patterns {
      margin-top: 30px;
      padding-top: 16px;
      border-top: 1px solid #2a2a46;
    }
    #patterns h2 { font-size: 18px; margin-bottom: 8px; }
    #patterns h3 { font-size: 15px; margin-top: 16px; margin-bottom: 4px; }
    #patterns p, #patterns li {
      font-size: 13px;
      color: #c0c0e0;
      line-height: 1.55;
    }
    footer {
      padding: 14px 34px 22px;
      border-top: 1px solid #262646;
      background: #050510;
      color: #9f9fcb;
      font-size: 11px;
    }
    @media (max-width: 800px) {
      header, .container, footer { padding-left: 16px; padding-right: 16px; }
      .controls { flex-direction: column; align-items: stretch; }
    }
  </style>
</head>
<body>
<header>
  <div class="badge">
    <span class="badge-dot"></span>
    <span>AI Chip &amp; Foundry Map · Edge · Datacenter · Outliers</span>
  </div>
  <h1>Silicon Power Map: Who Designs, Who Fabs, Who Owns Compute?</h1>
  <p class="subtitle">
    A compact but brutally honest map of AI chips: client NPUs, datacenter accelerators,
    their ASIC partners, foundries, nodes, wafer strategies, HBM stacks, and packaging.
    Filter, export, and update as new wafers hit the real world instead of slide decks.
  </p>
</header>

<div class="container">
  <div class="tabs">
    <button class="tab-btn active" data-target="client">1. Client / Edge SoCs</button>
    <button class="tab-btn" data-target="future">2. Future Datacenter Roadmap</button>
    <button class="tab-btn" data-target="datacenter">2.3 Current Datacenter Accelerators</button>
    <button class="tab-btn" data-target="non-tsmc">3. Non-TSMC &amp; Intel 18A Outliers</button>
  </div>

  <div class="controls">
    <input id="searchInput" class="search-input" placeholder="Search chip, designer, foundry, node, HBM, perf, etc.">
    <button id="exportBtn" class="export-btn">Export visible rows (CSV)</button>
  </div>
  <div class="hint">
    Filter works per active tab. Try things like “TSMC 3 nm”, “HBM3e”, “Broadcom ASIC”, “Gaudi”, “wafer-scale”, “18A”, “Samsung”.
  </div>

  <!-- 1. CLIENT / EDGE -->
  <section id="section-client" class="section active">
    <h2 class="section-title">1. Client / Edge AI SoCs (Apple, Google, Qualcomm)</h2>
    <p class="section-subtitle">
      1.1 Summary table – phones / tablets / laptops · Mapping A-/M-series, Tensor, and Snapdragon X/8 NPUs to their foundries and nodes.
    </p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th>Chip / Family</th>
            <th>Segment / Device</th>
            <th>Designer</th>
            <th>Foundry</th>
            <th>Node</th>
            <th>NPU / AI Block</th>
            <th>Headline Perf</th>
            <th>Wafer / Pkg</th>
            <th class="notes">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><span class="chip-pill">Apple A17 Pro</span></td>
            <td>iPhone 15 Pro</td>
            <td>Apple</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N3B (3 nm)</td>
            <td>16‑core Neural Engine</td>
            <td>~35 TOPS NPU</td>
            <td>300 mm; mobile SoC package</td>
            <td class="notes">
              First mass‑volume 3 nm phone chip; sets the baseline for on‑device vision and language on TSMC 3 nm.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Apple A18 / A18 Pro</span></td>
            <td>iPhone 16 family</td>
            <td>Apple</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N3E (3 nm)</td>
            <td>Next‑gen Neural Engine</td>
            <td>&gt;40–50 TOPS NPU (class)</td>
            <td>300 mm; refined N3E process</td>
            <td class="notes">
              Iteration on A17 with better perf/W; TSMC 3 nm remains Apple’s only reality for flagship phones.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Apple M4</span></td>
            <td>iPad Pro / Macs</td>
            <td>Apple</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N3E</td>
            <td>Desktop‑class Neural Engine</td>
            <td>≈38–45 TOPS NPU</td>
            <td>300 mm; large client SoC</td>
            <td class="notes">
              Unifies Apple’s “AI PC” story on 3 nm; still the same TSMC dependency, just in laptop clothing.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Google Tensor G3</span></td>
            <td>Pixel 8</td>
            <td>Google</td>
            <td><span class="tag-foundry samsung">Samsung</span></td>
            <td>4LPP (4 nm)</td>
            <td>TPU‑style NPU</td>
            <td>~10–15 TOPS class</td>
            <td>300 mm; Samsung EUV</td>
            <td class="notes">
              Early Tensor generations sacrificed peak efficiency for a Samsung partnership; good AI features, mediocre thermals vs TSMC peers.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Qualcomm Snapdragon 8 Gen 3</span></td>
            <td>Android flagships</td>
            <td>Qualcomm</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N4P (4 nm)</td>
            <td>Hexagon NPU + AI Engine</td>
            <td>&gt;40 TOPS “AI” (SoC)</td>
            <td>300 mm; mobile package</td>
            <td class="notes">
              Qualcomm quietly abandoned Samsung at the premium tier; another vote of no‑confidence in anything but TSMC for serious edge AI.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Snapdragon X Elite</span></td>
            <td>Windows AI PCs</td>
            <td>Qualcomm</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N4</td>
            <td>Hexagon NPU (PC)</td>
            <td>≈45 TOPS NPU; ≈75 total “AI”</td>
            <td>300 mm; laptop SoC</td>
            <td class="notes">
              The whole “AI PC” marketing wave is realistically just repackaged phone silicon, scaled up and still riding on TSMC wafers.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- 2. FUTURE DATACENTER ROADMAP -->
  <section id="section-future" class="section">
    <h2 class="section-title">2. Future Announced Parts: Rubin, Trainium3, Tesla AI5/AI6</h2>
    <p class="section-subtitle">
      2.1 Data center / cloud AI roadmap · 2.2 Tesla AI5 / AI6 dual‑foundry strategy.
      These chips will contend for the same limited advanced‑node wafer pool.
    </p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th>Sub‑Section</th>
            <th>Chip / Platform</th>
            <th>Segment / Role</th>
            <th>Designer</th>
            <th>ASIC Partner</th>
            <th>Foundry</th>
            <th>Node</th>
            <th>Wafer Strategy</th>
            <th>Perf (headline)</th>
            <th>HBM Type</th>
            <th>HBM Cap.</th>
            <th>HBM BW</th>
            <th>Interconnect</th>
            <th>Packaging</th>
            <th class="notes">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>2.1</td>
            <td><span class="chip-pill">NVIDIA Rubin</span></td>
            <td>Next‑gen GPU</td>
            <td>NVIDIA</td>
            <td>NVIDIA</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>3 nm / N2‑class (TBD)</td>
            <td>Reticle‑limited dies, 300 mm</td>
            <td>&gt;2× Blackwell FP8/FP4 (target)</td>
            <td>HBM3E / HBM4</td>
            <td>&gt;192 GB</td>
            <td>&gt;8 TB/s</td>
            <td>Next‑gen NVLink/NVSwitch</td>
            <td>CoWoS‑L</td>
            <td class="notes">
              Rubin is Blackwell with more of everything; still chained to the same TSMC capacity and CoWoS bottlenecks.
            </td>
          </tr>
          <tr>
            <td>2.1</td>
            <td><span class="chip-pill">AWS Trainium3</span></td>
            <td>Cloud training ASIC</td>
            <td>AWS / Annapurna</td>
            <td>Annapurna</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>3 nm‑class</td>
            <td>300 mm; multi‑die evolution</td>
            <td>2–4× Trn2 (PFLOP‑class)</td>
            <td>HBM3E</td>
            <td>&gt;128 GB</td>
            <td>&gt;5 TB/s</td>
            <td>NeuronLink + 400/800G</td>
            <td>Custom AWS modules</td>
            <td class="notes">
              AWS’s attempt to make “GPU optional” for most training; still ultimately fighting NVIDIA for 3 nm slots at TSMC.
            </td>
          </tr>
          <tr>
            <td>2.1</td>
            <td><span class="chip-pill">Google TPU v7 / Ironwood</span></td>
            <td>Hyper‑scale inference</td>
            <td>Google</td>
            <td>Broadcom (ASIC)</td>
            <td><span class="tag-foundry tsmc">TSMC (inferred)</span></td>
            <td>Advanced 3 nm‑class</td>
            <td>300 mm; dual‑die modules</td>
            <td>Multi‑PFLOP FP8 per chip; ExaFLOP‑scale pods</td>
            <td>HBM3E</td>
            <td>≈192 GB</td>
            <td>≈7+ TB/s</td>
            <td>Custom optical mesh</td>
            <td>Rack‑scale “AI hypercomputer”</td>
            <td class="notes">
              Google builds datacenter‑sized TPUs with optical switching instead of selling chips; the customer is really just Google itself.
            </td>
          </tr>
          <tr>
            <td>2.2</td>
            <td><span class="chip-pill">Tesla AI5 / AI6</span></td>
            <td>Next‑gen Tesla AI</td>
            <td>Tesla</td>
            <td>Tesla + foundries</td>
            <td><span class="tag-foundry samsung">Samsung</span> + <span class="tag-foundry tsmc">TSMC</span></td>
            <td>5 nm → 3 nm</td>
            <td>Conventional chips on 300 mm; multi‑module scaling</td>
            <td>Step‑function over Dojo (exact nums not public)</td>
            <td>HBM3 / 3E</td>
            <td>&gt;96–144 GB / module (class)</td>
            <td>Multi‑TB/s / module (class)</td>
            <td>Custom meshes + Ethernet</td>
            <td>Accelerator sleds, not wafer‑scale</td>
            <td class="notes">
              Instead of doubling‑down on wafer‑scale Dojo, Tesla is hedging across Samsung and TSMC with more conventional chips to derisk supply.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- 2.3 CURRENT DATACENTER ACCELERATORS -->
  <section id="section-datacenter" class="section">
    <h2 class="section-title">2.3 Current Datacenter AI Accelerators (Full Detail)</h2>
    <p class="section-subtitle">
      The chips already in the wild: H100, Blackwell, MI300X, Gaudi 3, TPU v4 / v5e / Trillium, Trainium / Inferentia, Tesla Dojo D1, Cerebras WSE‑3, Graphcore GC200/Bow.
    </p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th>Chip / Family</th>
            <th>Designer / IP</th>
            <th>ASIC / Impl.</th>
            <th>Foundry</th>
            <th>Node</th>
            <th>Segment</th>
            <th>Perf (headline)</th>
            <th>HBM Type</th>
            <th>HBM Cap.</th>
            <th>HBM BW</th>
            <th>Interconnect</th>
            <th>Wafer / Pkg</th>
            <th class="notes">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><span class="chip-pill">NVIDIA H100 (Hopper)</span></td>
            <td>NVIDIA</td>
            <td>NVIDIA</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>4N custom (4 nm)</td>
            <td>GPU – train + infer</td>
            <td>≈4 PFLOPS FP8, ≈2 PFLOPS FP16 per SXM</td>
            <td>HBM3</td>
            <td>80–94 GB</td>
            <td>≈3.0–3.35 TB/s</td>
            <td>NVLink 4 + NVSwitch</td>
            <td>300 mm; reticle‑limited die on CoWoS</td>
            <td class="notes">
              The workhorse of the current AI boom; other accelerators are mostly priced as “discounts” or “premiums” relative to H100 clusters.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">NVIDIA B200 / Blackwell</span></td>
            <td>NVIDIA</td>
            <td>NVIDIA</td>
            <td><span class="tag-foundry tsmc">TSMC (Taiwan + Arizona)</span></td>
            <td>4NP custom</td>
            <td>GPU – frontier training</td>
            <td>DGX: 72 PFLOPS FP8 / 144 PFLOPS FP4 (8 GPUs)</td>
            <td>HBM3E</td>
            <td>≈180 GB / GPU</td>
            <td>≈8 TB/s / GPU</td>
            <td>Next‑gen NVLink/NVSwitch + 800G</td>
            <td>300 mm; dual reticle‑limited dies stitched on CoWoS‑L</td>
            <td class="notes">
              Blackwell is essentially “H100 scaled until packaging screams”; it eats CoWoS capacity and forces everyone else to fight for leftover advanced packaging.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">AMD Instinct MI300X</span></td>
            <td>AMD</td>
            <td>AMD</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>5 nm + 6 nm chiplets</td>
            <td>GPU‑class accel</td>
            <td>≈1.3 PFLOPS BF16, ≈2.6 PFLOPS FP8</td>
            <td>HBM3</td>
            <td>192 GB</td>
            <td>≈5.3 TB/s</td>
            <td>Infinity Fabric</td>
            <td>300 mm; 3D stacked chiplets on interposer</td>
            <td class="notes">
              Less raw FP8 than Blackwell but more HBM per device; attractive where context window and batch size, not pure TFLOPs, dominate pain.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Intel Gaudi 3</span></td>
            <td>Intel (Habana)</td>
            <td>Intel + TSMC wafers</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>5 nm (compute)</td>
            <td>Accel – train + infer</td>
            <td>≈1.8 PFLOPS FP8/BF16</td>
            <td>HBM2E</td>
            <td>128 GB</td>
            <td>≈3.7 TB/s</td>
            <td>24× 200 GbE (9.6 Tb/s)</td>
            <td>300 mm; multi‑die with on‑pkg HBM</td>
            <td class="notes">
              Ethernet‑native scale‑out makes Gaudi interesting for operators who’d rather not pay NVLink tax, even if peak FLOPs lag.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Google TPU v4</span></td>
            <td>Google</td>
            <td>Broadcom (ASIC)</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>7 nm</td>
            <td>ASIC – train + infer</td>
            <td>≈275 TFLOPs BF16/INT8 per chip</td>
            <td>HBM2</td>
            <td>32 GB</td>
            <td>≈1.2 TB/s</td>
            <td>Custom 3D mesh in 4096‑chip pods</td>
            <td>300 mm; standard die</td>
            <td class="notes">
              TPU v4 is the invisible backbone of a lot of Google AI; you don’t buy it, you just rent the pod.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Google TPU v5e</span></td>
            <td>Google</td>
            <td>Broadcom</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>Adv. 7 nm‑class</td>
            <td>ASIC – cheaper train/infer</td>
            <td>≈197 TFLOPs BF16</td>
            <td>HBM2E</td>
            <td>16 GB</td>
            <td>≈0.8–0.9 TB/s</td>
            <td>TPU mesh</td>
            <td>300 mm; denser v4 derivative</td>
            <td class="notes">
              A deliberately “good enough” TPU tuned for cost‑per‑token instead of halo benchmarks.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Google TPU v6e “Trillium”</span></td>
            <td>Google</td>
            <td>Broadcom</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>New node (likely 5 nm‑class)</td>
            <td>ASIC – train + serve</td>
            <td>≈4.7× v5e compute</td>
            <td>HBM (upgraded)</td>
            <td>≈32 GB</td>
            <td>≈1.6 TB/s</td>
            <td>Doubling TPU ICI vs v5e</td>
            <td>300 mm; updated package</td>
            <td class="notes">
              Doubles both compute and memory vs v5e; this is where Google tries to stay competitive with H100/B200 without ever shipping chips.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">AWS Trainium (Trn1)</span></td>
            <td>AWS (Annapurna)</td>
            <td>Annapurna</td>
            <td><span class="tag-foundry tsmc">TSMC (consensus)</span></td>
            <td>7 nm‑class</td>
            <td>Accel – training</td>
            <td>Multi‑PFLOP FP16/BF16 per Trn1 node</td>
            <td>HBM2E‑class</td>
            <td>≈32 GB/chip (node aggregate higher)</td>
            <td>≈10 TB/s aggregate / node</td>
            <td>NeuronLink + 400/800G</td>
            <td>300 mm; multiple chips per Trn1</td>
            <td class="notes">
              Less glamorous than GPUs but good enough for many workloads at lower cost, locked to AWS as the landlord.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">AWS Inferentia2</span></td>
            <td>AWS</td>
            <td>Annapurna</td>
            <td><span class="tag-foundry tsmc">TSMC‑class</span></td>
            <td>7 nm‑class</td>
            <td>Accel – inference</td>
            <td>Up to ≈2.3 PFLOPS across largest Inf2 node</td>
            <td>HBM</td>
            <td>32 GB/chip</td>
            <td>≈9–10 TB/s aggregate</td>
            <td>NeuronLink + Ethernet</td>
            <td>300 mm; custom cards</td>
            <td class="notes">
              Designed to annihilate $/token for serving, not to impress benchmark slides; useful if you live fully inside AWS.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Tesla Dojo D1</span></td>
            <td>Tesla</td>
            <td>Tesla</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>7 nm (N7)</td>
            <td>Custom training chip</td>
            <td>ExaFLOP‑scale at system level</td>
            <td>External (no on‑pkg HBM)</td>
            <td>Tile‑level pools</td>
            <td>N/A per die</td>
            <td>On‑die mesh + tile fabric</td>
            <td>645 mm² dies on 300 mm wafers; tiles of many chips</td>
            <td class="notes">
              An audacious attempt at building a Tesla‑only training fabric; later partially walked back as the company hedged toward AI5/AI6 with big foundry partners.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Cerebras WSE‑3</span></td>
            <td>Cerebras</td>
            <td>Cerebras + TSMC</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>N5 (5 nm)</td>
            <td>Wafer‑scale engine</td>
            <td>≈125 PFLOPS AI compute per wafer</td>
            <td>On‑wafer SRAM (no HBM)</td>
            <td>44 GB on‑chip; external memory in system</td>
            <td>PB/s‑class on‑wafer fabric</td>
            <td>One 46,255 mm² “chip” per 300 mm wafer</td>
            <td class="notes">
              The extreme: one wafer equals one giant chip; great for highly regular, batched workloads if you can live in Cerebras’s ecosystem.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Graphcore GC200 / Bow IPU</span></td>
            <td>Graphcore</td>
            <td>Graphcore + TSMC</td>
            <td><span class="tag-foundry tsmc">TSMC</span></td>
            <td>7 nm</td>
            <td>IPU accelerator</td>
            <td>≈250 TOPS FP16 per IPU</td>
            <td>On‑chip SRAM + external DRAM</td>
            <td>0.9 GB on‑chip; rest off‑chip</td>
            <td>2.8 Tb/s internal fabric</td>
            <td>Standard 300 mm die; wafer‑on‑wafer for Bow</td>
            <td class="notes">
              Focuses on thousands of small cores with tightly coupled SRAM instead of fat HBM stacks – intellectually interesting, commercially squeezed.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- 3. NON-TSMC OUTLIERS -->
  <section id="section-non-tsmc" class="section">
    <h2 class="section-title">3. Non‑TSMC Outliers – Samsung‑fabbed AI &amp; Intel 18A</h2>
    <p class="section-subtitle">
      3.1 Samsung‑fabbed AI silicon (Tesla + mobile) · 3.2 Intel’s 18A‑era tiles. These are the escape hatches if TSMC wafers or geopolitics go sideways.
    </p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th>Chip / Family</th>
            <th>Segment</th>
            <th>Designer</th>
            <th>Foundry</th>
            <th>Node</th>
            <th>AI Block</th>
            <th>Memory</th>
            <th>Packaging</th>
            <th class="notes">Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><span class="chip-pill">Tesla FSD (HW3/HW4/HW5)</span></td>
            <td>Auto inference</td>
            <td>Tesla</td>
            <td><span class="tag-foundry samsung">Samsung</span></td>
            <td>14 nm → 7/4 nm</td>
            <td>Custom Tesla neural accelerators</td>
            <td>LPDDR / GDDR‑class</td>
            <td>Automotive packages</td>
            <td class="notes">
              Proof that Samsung can deliver safety‑critical AI silicon at scale; also proof Tesla never fully trusted a single foundry.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Google Tensor G1–G4</span></td>
            <td>Pixel phones</td>
            <td>Google</td>
            <td><span class="tag-foundry samsung">Samsung</span></td>
            <td>5 nm → 4 nm</td>
            <td>TPU‑style NPU</td>
            <td>LPDDR</td>
            <td>Mobile packages</td>
            <td class="notes">
              A whole era of Google silicon that lived on Samsung; efficient enough for features, but ultimately not competitive with TSMC designs.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Samsung Exynos AI SoCs</span></td>
            <td>Mobile client AI</td>
            <td>Samsung</td>
            <td><span class="tag-foundry samsung">Samsung</span></td>
            <td>4LPP → 3 nm GAA</td>
            <td>Samsung NPU + DSP</td>
            <td>LPDDR5/5X</td>
            <td>Standard mobile</td>
            <td class="notes">
              When Samsung wins with Exynos, it wins twice – product margin and foundry margin. When it loses, it hands that power straight to TSMC and Qualcomm.
            </td>
          </tr>
          <tr>
            <td><span class="chip-pill">Intel 18A tiles (future AI GPUs / XPUs)</span></td>
            <td>Datacenter AI &amp; foundry customers</td>
            <td>Intel + partners</td>
            <td><span class="tag-foundry intel">Intel (IFS)</span></td>
            <td>18A (~1.8 nm marketing)</td>
            <td>Matrix engines / NPUs</td>
            <td>HBM3E + DDR/CXL</td>
            <td>EMIB + Foveros 3D chiplets</td>
            <td class="notes">
              Intel’s shot at becoming the “non‑TSMC” home for big AI tiles. If 18A hits yield and perf, this row goes from curiosity to power pivot.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- PATTERNS -->
  <section id="patterns">
    <h2>Key Patterns &amp; Guidance</h2>

    <h3>1. Almost every meaningful AI chip resolves to TSMC wafers.</h3>
    <p>
      H100, Blackwell, MI300X, Gaudi 3, TPU v4/v5/v6/v7, Trainium, Cerebras WSE‑3, Graphcore GC200, Apple A/M, Snapdragon 8/X –
      they’re all different logos on the same handful of advanced TSMC nodes. Your “diversified” silicon strategy is often monoculture at the foundry layer.
    </p>

    <h3>2. FLOPs are abundant; HBM bandwidth and packaging are the choke points.</h3>
    <p>
      Whoever controls CoWoS capacity and HBM3E supply controls practical AI throughput. GPUs are now mostly constrained by memory bandwidth and how many stacks you can bolt onto a substrate without melting it.
    </p>

    <h3>3. Hyperscalers are de‑risking NVIDIA, not de‑risking TSMC.</h3>
    <p>
      AWS, Google, Tesla and friends moved to in‑house ASICs to avoid GPU pricing and roadmap politics. But almost all of them still line up at the same TSMC docks. The single point of failure just moved one layer down the stack.
    </p>

    <h3>4. Samsung and Intel 18A are not “better”; they are “necessary”.</h3>
    <p>
      Even if they lag a node or two in perf/W, they matter because “slightly worse compute” beats “no wafers at all”. The real game is not winning benchmarks – it’s securing enough non‑TSMC capacity to keep scaling when everyone else stalls.
    </p>

    <h3>5. Practical filter: how to evaluate any new “AI chip”.</h3>
    <ul>
      <li>Who designs it, and who actually fabs it?</li>
      <li>On what node, with what HBM type/capacity/bandwidth?</li>
      <li>Through which interconnect can you realistically scale it in your racks?</li>
      <li>And what happens to your roadmap if that <em>one</em> foundry goes offline or reprioritizes another customer?</li>
    </ul>

    <p>
      Ignore those questions, and you’re not doing AI strategy – you’re just buying shiny PCIe cards and praying the supply chain holds. In a world where intelligence is capped by wafers, the real leverage sits with whoever can see (and hedge) the foundry layer clearly.
    </p>
  </section>
</div>

<footer>
  v3 · Optimized width · Wider notes · All tables exportable as CSV. Update the rows as new nodes and chips appear; the physics of wafers will not negotiate with your roadmap.
</footer>

<script>
  (function() {
    const tabs = document.querySelectorAll(".tab-btn");
    const sections = {
      client: document.getElementById("section-client"),
      future: document.getElementById("section-future"),
      datacenter: document.getElementById("section-datacenter"),
      "non-tsmc": document.getElementById("section-non-tsmc"),
    };
    const searchInput = document.getElementById("searchInput");
    const exportBtn = document.getElementById("exportBtn");
    let activeKey = "client";

    function setActive(key) {
      activeKey = key;
      tabs.forEach(btn => {
        btn.classList.toggle("active", btn.dataset.target === key);
      });
      Object.entries(sections).forEach(([k, sec]) => {
        sec.classList.toggle("active", k === key);
      });
      searchInput.value = "";
      applyFilter("");
    }

    function currentTbody() {
      const sec = sections[activeKey];
      if (!sec) return null;
      const table = sec.querySelector("table");
      return table ? table.querySelector("tbody") : null;
    }

    function applyFilter(q) {
      const tbody = currentTbody();
      if (!tbody) return;
      const query = (q || "").toLowerCase();
      tbody.querySelectorAll("tr").forEach(tr => {
        if (!query) {
          tr.style.display = "";
        } else {
          const text = tr.innerText.toLowerCase();
          tr.style.display = text.includes(query) ? "" : "none";
        }
      });
    }

    function exportCSV() {
      const sec = sections[activeKey];
      if (!sec) return;
      const table = sec.querySelector("table");
      if (!table) return;

      const rows = Array.from(table.querySelectorAll("tr"));
      const csv = rows.map(row => {
        const cells = Array.from(row.querySelectorAll("th,td"));
        return cells
          .map(cell => {
            const text = cell.innerText.replace(/\s+/g, " ").trim();
            const escaped = text.replace(/"/g, '""');
            return `"${escaped}"`;
          })
          .join(",");
      }).join("\n");

      const blob = new Blob([csv], { type: "text/csv;charset=utf-8;" });
      const url = URL.createObjectURL(blob);
      const a = document.createElement("a");
      a.href = url;
      a.download = `ai_chip_foundry_map_${activeKey}.csv`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
    }

    tabs.forEach(btn => {
      btn.addEventListener("click", () => setActive(btn.dataset.target));
    });
    searchInput.addEventListener("input", e => applyFilter(e.target.value));
    exportBtn.addEventListener("click", exportCSV);
  })();
</script>
</body>
</html>
